{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.vdms import VDMS_Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "# Folder with pdf and extracted images\n",
    "datapath = Path(\"./multimodal_files\").resolve()\n",
    "datapath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_url = \"https://www.infoblox.com/wp-content/uploads/infoblox-deployment-infoblox-rest-api.pdf\"\n",
    "pdf_path = str(datapath / pdf_url.split(\"/\")[-1])\n",
    "# with open(pdf_path, \"wb\") as f:\n",
    "#     f.write(requests.get(pdf_url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = str(datapath / pdf_url.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\redzh\\\\Documents\\\\myStuff\\\\Tests\\\\PDF_IMG_RAG\\\\multimodal_files\\\\infoblox-deployment-infoblox-rest-api.pdf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path\n",
    "#conda install -c conda-forge poppler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Set the TESSDATA_PREFIX environment variable\n",
    "os.environ['TESSDATA_PREFIX'] = r'C:\\Users\\redzh\\Downloads'\n",
    "\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=pdf_path,\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=datapath,\n",
    ")\n",
    "\n",
    "datapath = str(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unstructured\n",
    "\n",
    "tables = []\n",
    "texts = []\n",
    "for element in raw_pdf_elements:\n",
    "    if isinstance(element, unstructured.documents.elements.Table):\n",
    "        tables.append(element)\n",
    "    elif isinstance(element, unstructured.documents.elements.Text) or isinstance(element, unstructured.documents.elements.NarrativeText) or isinstance(element, unstructured.documents.elements.Title):\n",
    "        texts.append(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.vdms import VDMS_Client\n",
    "#! docker run --rm -d -p 55559:55555 --name vdms_rag_nb intellabs/vdms:latest\n",
    "vdms_client = VDMS_Client(port=55559)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "from langchain_community.vectorstores import VDMS\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Creating OpenCLIPEmbeddings instance...\")\n",
    "# Create OpenCLIPEmbeddings instance\n",
    "clip_embeddings = OpenCLIPEmbeddings(\n",
    "    model_name=\"ViT-g-14\", checkpoint=\"laion2b_s34b_b88k\"\n",
    ")\n",
    "logger.info(\"OpenCLIPEmbeddings instance created successfully.\")\n",
    "\n",
    "logger.info(\"Creating VDMS...\")\n",
    "# Create VDMS\n",
    "vectorstore = VDMS(\n",
    "    client=vdms_client,\n",
    "    collection_name=\"mm_rag_clip_photos\",\n",
    "    embedding=clip_embeddings\n",
    ")\n",
    "\n",
    "logger.info(\"VDMS created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(datapath)\n",
    "logger.info(f\"Found {len(files)} files in datapath.\")\n",
    "\n",
    "logger.info(\"Filtering for .jpg files...\")\n",
    "\n",
    "jpg_files = []\n",
    "for file in files:\n",
    "    if file.endswith(\".jpg\"):\n",
    "        jpg_files.append(file)\n",
    "        logger.debug(f\"Added {file} to jpg_files list.\")\n",
    "    else:\n",
    "        logger.debug(f\"Skipped non-jpg file: {file}\")\n",
    "logger.info(f\"Found {len(jpg_files)} .jpg files.\")\n",
    "\n",
    "try:\n",
    "    image_uris = [os.path.join(datapath, jpg_file) for jpg_file in jpg_files]\n",
    "    logger.info(f\"Created {len(image_uris)} image URIs.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating image URIs: {str(e)}\")\n",
    "    image_uris = []\n",
    "\n",
    "logger.info(\"Sorting image URIs...\")\n",
    "image_uris = sorted(image_uris)\n",
    "logger.info(\"Image URIs sorted.\")\n",
    "logger.info(f\"Found {len(image_uris)} image URIs with .jpg extension.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add images\n",
    "if image_uris:\n",
    "    vectorstore.add_images(uris=image_uris)\n",
    "    logger.info(f\"Added {len(image_uris)} images to vectorstore.\")\n",
    "else:\n",
    "    logger.warning(\"No image URIs found. Skipping adding images to vectorstore.\")\n",
    "\n",
    "if texts:\n",
    "    logger.info(\"Adding documents to vectorstore...\")\n",
    "    text_contents = [text.text for text in texts]  # Extract text content\n",
    "    vectorstore.add_texts(texts=text_contents)\n",
    "    logger.info(\"Documents added to vectorstore successfully.\")\n",
    "else:\n",
    "    logger.warning(\"No texts found. Skipping adding documents to vectorstore.\")\n",
    "\n",
    "logger.info(\"Creating retriever...\")\n",
    "# Make retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "logger.info(\"Retriever created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG\n",
    "\n",
    "# `vectorstore.add_images` will store / retrieve images as base64 encoded strings.\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string.\n",
    "\n",
    "    Args:\n",
    "    base64_string (str): Base64 string of the original image.\n",
    "    size (tuple): Desired size of the image as (width, height).\n",
    "\n",
    "    Returns:\n",
    "    str: Base64 string of the resized image.\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def is_base64(s):\n",
    "    \"\"\"Check if a string is Base64 encoded\"\"\"\n",
    "    try:\n",
    "        return base64.b64encode(base64.b64decode(s)) == s.encode()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"Split numpy array images and texts\"\"\"\n",
    "    images = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        doc = doc.page_content  # Extract Document contents\n",
    "        if is_base64(doc):\n",
    "            # Resize image to avoid OAI server error\n",
    "            images.append(\n",
    "                resize_base64_image(doc, size=(250, 250))\n",
    "            )  # base64 encoded str\n",
    "        else:\n",
    "            text.append(doc)\n",
    "    return {\"images\": images, \"texts\": text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "def prompt_func(data_dict):\n",
    "    # Joining the context texts into a single string\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        image_message = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{data_dict['context']['images'][0]}\"\n",
    "            },\n",
    "        }\n",
    "        messages.append(image_message)\n",
    "\n",
    "    # Adding the text message for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"As an expert in Infoblox and API  and Python, your task is to design and document APIs, \"\n",
    "            \"ensuring they are comprehensive, accurate, and user-friendly. You will use Python to script \"\n",
    "            \"interactions with these APIs and validate their functionality. Your documentation should include:\\n\"\n",
    "            \"- Detailed descriptions of API endpoints, including parameters, request bodies, and responses.\\n\"\n",
    "            \"- Examples of Python scripts that interact with these endpoints.\\n\"\n",
    "            \"- Explanation of authentication methods used by the API.\\n\"\n",
    "            \"- Best practices for error handling and versioning in API design.\\n\\n\"\n",
    "            f\"User-provided keywords: {data_dict['question']}\\n\\n\"\n",
    "            \"API Documentation and Python Scripts:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"Multi-modal RAG chain\"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    llm_model = Ollama(\n",
    "        verbose=True, temperature=0.5, model=\"llava\", base_url=\"http://localhost:11434\"\n",
    "    )\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(prompt_func)\n",
    "        | llm_model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query = \"What is a network?\"\n",
    "docs = retriever.invoke(query, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Add a network', metadata={'id': '9c0b290d-326e-44a7-b18e-68fae5e26948'}),\n",
       " Document(page_content='Search for a network', metadata={'id': 'c0257225-c797-44cb-8808-c2271d80979f'}),\n",
       " Document(page_content='Search for a network', metadata={'id': 'a8d2e8a7-38c7-4f54-8854-6cb5f2299727'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I can help with that. Here's an example of API documentation and Python scripts for Infoblox:\n",
      "\n",
      "API Documentation:\n",
      "-------------\n",
      "\n",
      "### Add a network\n",
      "\n",
      "#### Request\n",
      "\n",
      "* Method: POST\n",
      "* Endpoint: /networks\n",
      "* Parameters:\n",
      "\t+ name (required): The name of the network.\n",
      "\t+ ip\\_range (required): The IP range of the network.\n",
      "\t* Example request body:\n",
      "```json\n",
      "{\n",
      "    \"name\": \"My Network\",\n",
      "    \"ip_range\": \"10.0.0/24\"\n",
      "}\n",
      "```\n",
      "#### Response\n",
      "\n",
      "* Status code: 201 Created\n",
      "* Body: The newly created network object.\n",
      "\n",
      "### Search for a network\n",
      "\n",
      "#### Request\n",
      "\n",
      "* Method: GET\n",
      "* Endpoint: /networks\n",
      "* Parameters:\n",
      "\t+ q (optional): A search query.\n",
      "\t* Example request:\n",
      "```bash\n",
      "https://<hostname>/wapi/v1.0/networks?q=My Network\n",
      "```\n",
      "#### Response\n",
      "\n",
      "* Status code: 200 OK\n",
      "* Body: An array of network objects that match the search criteria.\n",
      "\n",
      "Authentication Method:\n",
      "---------------------\n",
      "\n",
      "Infoblox uses API keys for authentication. To obtain an API key, you need to create a user account on the Infoblox appliance and generate an API key. You can then use this API key in your API requests by including it in the request header with the key \"Authorization\" and value \"API\\_KEY\".\n",
      "\n",
      "Python Scripts:\n",
      "-----------------\n",
      "```python\n",
      "import requests\n",
      "import json\n",
      "\n",
      "# Add a network\n",
      "def add_network(name, ip_range):\n",
      "    url = \"https://<hostname>/wapi/v1.0/networks\"\n",
      "    headers = {\"Authorization\": \"<API\\_KEY>\"}\n",
      "    payload = {\n",
      "        \"name\": name,\n",
      "        \"ip_range\": ip_range\n",
      "    }\n",
      "    response = requests.post(url, headers=headers, json=payload)\n",
      "    if response.status_code == 201:\n",
      "        return json.loads(response.text)[\"data\"]\n",
      "    else:\n",
      "        raise Exception(\"Failed to add network\")\n",
      "\n",
      "# Search for a network\n",
      "def search_networks(q):\n",
      "    url = \"https://<hostname>/wapi/v1.0/networks\"\n",
      "    headers = {\"Authorization\": \"<API\\_KEY>\"}\n",
      "    params = {\"q\": q}\n",
      "    response = requests.get(url, headers=headers, params=params)\n",
      "    if response.status_code == 200:\n",
      "        return json.loads(response.text)[\"data\"]\n",
      "    else:\n",
      "        raise Exception(\"Failed to search networks\")\n",
      "```\n",
      "Error Handling and Versioning Best Practices:\n",
      "----------------------------------------\n",
      "\n",
      "* Use appropriate status codes for API responses. For example, use 201 for created resources, 200 for successful requests, and 400 for bad requests.\n",
      "* Include error messages in API responses to help users understand what went wrong.\n",
      "* Version the API by including a version number in the request header or URL. This can help prevent breaking changes in future versions of the API.\n",
      "* Document any known issues or limitations with the API. \n"
     ]
    }
   ],
   "source": [
    "chain = multi_modal_rag_chain(retriever)\n",
    "response = chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! docker kill vdms_rag_nb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teleprompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
